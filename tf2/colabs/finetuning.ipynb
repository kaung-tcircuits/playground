{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaung-tcircuits/playground/blob/simclr_detect/tf2/colabs/finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89hdk3d7N-hb"
      },
      "source": [
        "##### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Colab preparation\n",
        "\n",
        "Here I am creating the data folders for the training. In order to upload the actual data annotations and images, one will have to download, https://app.roboflow.com/mlexercises/firefighting-device-detection-yeetx/1 in Tensorflow\n",
        "Object Detection Dataset format and upload to the folders accordingly."
      ],
      "metadata": {
        "id": "DFDGYEEkpRf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IbX4a6NKOBTr"
      },
      "outputs": [],
      "source": [
        "# !mkdir /content/Firefighting_Device_Detection\n",
        "!mkdir -p /content/Firefighting_Device_Detection/train\n",
        "!mkdir -p /content/Firefighting_Device_Detection/test\n",
        "!mkdir -p /content/Firefighting_Device_Detection/valid\n",
        "\n",
        "# Upload the data after running the cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing Data\n",
        "\n",
        "Here, I am simply looking at the class distribution in the dataset and the box statistics to have a general understanding of my data."
      ],
      "metadata": {
        "id": "9fmW9G-fo66w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "import os\n",
        "\n",
        "def analyze_split(df, split_name):\n",
        "    \"\"\"Analyze a single split of the dataset\"\"\"\n",
        "    print(f\"\\n===== {split_name.upper()} Split Analysis =====\")\n",
        "\n",
        "    n_images = df['filename'].nunique()\n",
        "    n_annotations = len(df)\n",
        "    n_classes = df['class'].nunique()\n",
        "\n",
        "    print(f\"Total images: {n_images}\")\n",
        "    print(f\"Total annotations: {n_annotations}\")\n",
        "    print(f\"Total classes: {n_classes}\")\n",
        "    print(f\"Average annotations per image: {n_annotations/n_images:.1f}\")\n",
        "\n",
        "    # Class distribution\n",
        "    print(\"\\nClass distribution (sorted by frequency):\")\n",
        "    class_dist = df['class'].value_counts()\n",
        "    for class_name, count in class_dist.items():\n",
        "        percentage = (count/n_annotations) * 100\n",
        "        print(f\"{class_name}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Some size statistics\n",
        "    print(\"\\nBounding Box Statistics (pixels):\")\n",
        "    print(f\"Average width: {df['box_width'].mean():.1f}\")\n",
        "    print(f\"Average height: {df['box_height'].mean():.1f}\")\n",
        "    print(f\"Average area: {df['box_area'].mean():.1f}\")\n",
        "\n",
        "    print(\"\\nRelative Box Statistics (% of image):\")\n",
        "    print(f\"Average width: {(df['relative_width'].mean()*100):.1f}%\")\n",
        "    print(f\"Average height: {(df['relative_height'].mean()*100):.1f}%\")\n",
        "    print(f\"Average area: {(df['relative_area'].mean()*100):.1f}%\")\n",
        "\n",
        "base_path = '/content/Firefighting_Device_Detection'\n",
        "splits = ['train', 'test', 'valid']\n",
        "\n",
        "columns = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "\n",
        "dfs = {}\n",
        "for split in splits:\n",
        "    path = os.path.join(base_path, split, '_annotations.csv')\n",
        "    df = pd.read_csv(path, names=columns, skiprows=1)\n",
        "    for col in ['width', 'height', 'xmin', 'ymin', 'xmax', 'ymax']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='raise')\n",
        "    df['split'] = split\n",
        "\n",
        "    df['box_width'] = df['xmax'] - df['xmin']\n",
        "    df['box_height'] = df['ymax'] - df['ymin']\n",
        "    df['box_area'] = df['box_width'] * df['box_height']\n",
        "    df['box_aspect_ratio'] = df['box_width'] / df['box_height']\n",
        "    df['relative_width'] = df['box_width'] / df['width']\n",
        "    df['relative_height'] = df['box_height'] / df['height']\n",
        "    df['relative_area'] = df['box_area'] / (df['width'] * df['height'])\n",
        "\n",
        "    dfs[split] = df\n",
        "\n",
        "    analyze_split(df, split)\n",
        "\n",
        "# df_all = pd.concat(dfs.values(), ignore_index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1SZbo25GaMy",
        "outputId": "a6ca0c80-6adc-4083-8398-3ef97dd2a3c1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== TRAIN Split Analysis =====\n",
            "Total images: 102\n",
            "Total annotations: 2606\n",
            "Total classes: 40\n",
            "Average annotations per image: 25.5\n",
            "\n",
            "Class distribution (sorted by frequency):\n",
            "24V-power-cord: 940 (36.1%)\n",
            "fire-fan-manual-control-line: 341 (13.1%)\n",
            "i-o-module: 151 (5.8%)\n",
            "bus-isolation-module: 136 (5.2%)\n",
            "coded-smoke-detector: 133 (5.1%)\n",
            "fire-hydrant-button: 103 (4.0%)\n",
            "acousto-optic-alarm: 93 (3.6%)\n",
            "manual-alarm-button-with-fire-telephone-jack: 86 (3.3%)\n",
            "manual-automatic-switching-device: 60 (2.3%)\n",
            "coded-temperature-detector: 58 (2.2%)\n",
            "input-module: 39 (1.5%)\n",
            "normally-open-smoke-exhaust-valve-with-280-operation: 35 (1.3%)\n",
            "fire-broadcasting-line: 34 (1.3%)\n",
            "secondary-fire-shutter-door-control-box: 33 (1.3%)\n",
            "dedicated-metal-module-box-for-fire-smoke-exhaust-fan: 31 (1.2%)\n",
            "fire-water-pump-manual-control-line: 30 (1.2%)\n",
            "light-display: 30 (1.2%)\n",
            "security-video-intercom-door-machine: 25 (1.0%)\n",
            "normally-open-smoke-exhaust-valve-with-70-operation: 25 (1.0%)\n",
            "dedicated-metal-module-box-for-fire-supplementary-fan: 23 (0.9%)\n",
            "safety-signal-valve: 18 (0.7%)\n",
            "water-flow-indicator: 18 (0.7%)\n",
            "speaker: 16 (0.6%)\n",
            "area-display: 16 (0.6%)\n",
            "fire-equipment-power-monitoring-line: 15 (0.6%)\n",
            "voltage-signal-sensor: 14 (0.5%)\n",
            "fire-telephone-extension: 14 (0.5%)\n",
            "video-intercom-card-reader: 13 (0.5%)\n",
            "gun-type-infrared-camera-in-the-basement: 12 (0.5%)\n",
            "metal-modular-box: 8 (0.3%)\n",
            "gas-spray-audible-and-visual-alarm: 7 (0.3%)\n",
            "smoke-vent: 7 (0.3%)\n",
            "pressure-switch-flow-switch-start-pump-line: 7 (0.3%)\n",
            "deflation-indicator-light: 7 (0.3%)\n",
            "the-electromagnetic-valve: 7 (0.3%)\n",
            "pressure-switch-gas-extinguisher: 7 (0.3%)\n",
            "electrical-fire-monitoring-line: 4 (0.2%)\n",
            "normally-open-smoke-exhaust-valve-with-70-operation-closed-in-case-of-fire: 4 (0.2%)\n",
            "emergency-manual-start-stop-button: 4 (0.2%)\n",
            "dedicated-metal-module-box-for-fire-pump: 2 (0.1%)\n",
            "\n",
            "Bounding Box Statistics (pixels):\n",
            "Average width: 36.1\n",
            "Average height: 31.3\n",
            "Average area: 1204.2\n",
            "\n",
            "Relative Box Statistics (% of image):\n",
            "Average width: 4.4%\n",
            "Average height: 4.6%\n",
            "Average area: 0.2%\n",
            "\n",
            "===== TEST Split Analysis =====\n",
            "Total images: 18\n",
            "Total annotations: 424\n",
            "Total classes: 30\n",
            "Average annotations per image: 23.6\n",
            "\n",
            "Class distribution (sorted by frequency):\n",
            "24V-power-cord: 149 (35.1%)\n",
            "fire-fan-manual-control-line: 59 (13.9%)\n",
            "coded-smoke-detector: 25 (5.9%)\n",
            "bus-isolation-module: 25 (5.9%)\n",
            "i-o-module: 22 (5.2%)\n",
            "fire-hydrant-button: 19 (4.5%)\n",
            "acousto-optic-alarm: 17 (4.0%)\n",
            "manual-alarm-button-with-fire-telephone-jack: 16 (3.8%)\n",
            "manual-automatic-switching-device: 13 (3.1%)\n",
            "fire-equipment-power-monitoring-line: 9 (2.1%)\n",
            "normally-open-smoke-exhaust-valve-with-70-operation: 9 (2.1%)\n",
            "input-module: 7 (1.7%)\n",
            "normally-open-smoke-exhaust-valve-with-280-operation: 5 (1.2%)\n",
            "dedicated-metal-module-box-for-fire-smoke-exhaust-fan: 5 (1.2%)\n",
            "fire-telephone-extension: 5 (1.2%)\n",
            "dedicated-metal-module-box-for-fire-supplementary-fan: 5 (1.2%)\n",
            "security-video-intercom-door-machine: 4 (0.9%)\n",
            "voltage-signal-sensor: 4 (0.9%)\n",
            "coded-temperature-detector: 4 (0.9%)\n",
            "normally-open-smoke-exhaust-valve-with-70-operation-closed-in-case-of-fire: 3 (0.7%)\n",
            "area-display: 3 (0.7%)\n",
            "emergency-manual-start-stop-button: 3 (0.7%)\n",
            "light-display: 3 (0.7%)\n",
            "safety-signal-valve: 2 (0.5%)\n",
            "water-flow-indicator: 2 (0.5%)\n",
            "secondary-fire-shutter-door-control-box: 2 (0.5%)\n",
            "the-electromagnetic-valve: 1 (0.2%)\n",
            "deflation-indicator-light: 1 (0.2%)\n",
            "pressure-switch-gas-extinguisher: 1 (0.2%)\n",
            "gas-spray-audible-and-visual-alarm: 1 (0.2%)\n",
            "\n",
            "Bounding Box Statistics (pixels):\n",
            "Average width: 38.0\n",
            "Average height: 33.2\n",
            "Average area: 1355.8\n",
            "\n",
            "Relative Box Statistics (% of image):\n",
            "Average width: 4.7%\n",
            "Average height: 4.7%\n",
            "Average area: 0.2%\n",
            "\n",
            "===== VALID Split Analysis =====\n",
            "Total images: 28\n",
            "Total annotations: 755\n",
            "Total classes: 33\n",
            "Average annotations per image: 27.0\n",
            "\n",
            "Class distribution (sorted by frequency):\n",
            "24V-power-cord: 295 (39.1%)\n",
            "fire-fan-manual-control-line: 74 (9.8%)\n",
            "bus-isolation-module: 48 (6.4%)\n",
            "coded-smoke-detector: 42 (5.6%)\n",
            "i-o-module: 37 (4.9%)\n",
            "fire-hydrant-button: 35 (4.6%)\n",
            "acousto-optic-alarm: 34 (4.5%)\n",
            "manual-alarm-button-with-fire-telephone-jack: 34 (4.5%)\n",
            "coded-temperature-detector: 25 (3.3%)\n",
            "manual-automatic-switching-device: 13 (1.7%)\n",
            "fire-equipment-power-monitoring-line: 12 (1.6%)\n",
            "voltage-signal-sensor: 12 (1.6%)\n",
            "secondary-fire-shutter-door-control-box: 12 (1.6%)\n",
            "fire-broadcasting-line: 10 (1.3%)\n",
            "security-video-intercom-door-machine: 9 (1.2%)\n",
            "input-module: 9 (1.2%)\n",
            "dedicated-metal-module-box-for-fire-supplementary-fan: 7 (0.9%)\n",
            "light-display: 7 (0.9%)\n",
            "gun-type-infrared-camera-in-the-basement: 6 (0.8%)\n",
            "normally-open-smoke-exhaust-valve-with-280-operation: 5 (0.7%)\n",
            "normally-open-smoke-exhaust-valve-with-70-operation: 5 (0.7%)\n",
            "dedicated-metal-module-box-for-fire-smoke-exhaust-fan: 5 (0.7%)\n",
            "speaker: 5 (0.7%)\n",
            "electrical-fire-monitoring-line: 3 (0.4%)\n",
            "fire-telephone-extension: 2 (0.3%)\n",
            "area-display: 2 (0.3%)\n",
            "normally-open-smoke-exhaust-valve-with-70-operation-closed-in-case-of-fire: 1 (0.1%)\n",
            "safety-signal-valve: 1 (0.1%)\n",
            "water-flow-indicator: 1 (0.1%)\n",
            "fire-water-pump-manual-control-line: 1 (0.1%)\n",
            "emergency-manual-start-stop-button: 1 (0.1%)\n",
            "explosion-proof-smoke-detector: 1 (0.1%)\n",
            "pressure-switch-flow-switch-start-pump-line: 1 (0.1%)\n",
            "\n",
            "Bounding Box Statistics (pixels):\n",
            "Average width: 35.3\n",
            "Average height: 30.8\n",
            "Average area: 1137.9\n",
            "\n",
            "Relative Box Statistics (% of image):\n",
            "Average width: 4.3%\n",
            "Average height: 4.6%\n",
            "Average area: 0.2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing SimCLR backbone model\n",
        "\n",
        "Here, I just simply load the model and give it a sample input to analyze the spatial reduction."
      ],
      "metadata": {
        "id": "qwYB0K1joo1I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYNno0xtOFJ-"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-research/simclr/blob/master/tf2/colabs/finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def inspect_simclr_model(base_model):\n",
        "    sample_input = tf.zeros([1, 224, 224, 3])\n",
        "    outputs = base_model(sample_input, trainable=False)\n",
        "\n",
        "    print(\"=== SimCLR Output Analysis ===\\n\")\n",
        "\n",
        "    print(\"\\nSpatial Reduction Path:\")\n",
        "    spatial_features = []\n",
        "    for name, tensor in outputs.items():\n",
        "        if len(tensor.shape) == 4:\n",
        "            spatial_features.append((name, - tensor.shape[1] * tensor.shape[2], tensor.shape[3], tensor.shape))\n",
        "        if len(tensor.shape) == 2:\n",
        "            spatial_features.append((name, float('inf'), - tensor.shape[1], tensor.shape))\n",
        "\n",
        "    spatial_features.sort(key=lambda x: (x[1], x[2]))\n",
        "\n",
        "    for name, resolution, channels, shape in spatial_features:\n",
        "        print(f\"{name:<15} shape: {shape} \")\n",
        "\n",
        "\n",
        "base_model = hub.load(\"gs://simclr-checkpoints-tf2/simclrv2/pretrained/r50_1x_sk0/saved_model/\")\n",
        "inspect_simclr_model(base_model)"
      ],
      "metadata": {
        "id": "5plHiE2RpVVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794d16f7-0d33-47c6-9588-f2f38aac8963"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_42_layer_call_and_return_conditional_losses_34851) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_48_layer_call_and_return_conditional_losses_22359) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_25_layer_call_and_return_conditional_losses_32777) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_20_layer_call_and_return_conditional_losses_16199) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_30_layer_call_and_return_conditional_losses_18399) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_layer_call_and_return_conditional_losses_29402) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_33_layer_call_and_return_conditional_losses_19059) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_21_layer_call_and_return_conditional_losses_16419) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_25_layer_call_and_return_conditional_losses_17299) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_10_layer_call_and_return_conditional_losses_30947) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_31_layer_call_and_return_conditional_losses_33509) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_42_layer_call_and_return_conditional_losses_21039) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_40_layer_call_and_return_conditional_losses_20599) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_6_layer_call_and_return_conditional_losses_30459) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_24_layer_call_and_return_conditional_losses_32655) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_28_layer_call_and_return_conditional_losses_17959) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_11_layer_call_and_return_conditional_losses_14219) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_29_layer_call_and_return_conditional_losses_18179) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_44_layer_call_and_return_conditional_losses_35095) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_23_layer_call_and_return_conditional_losses_16859) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_24_layer_call_and_return_conditional_losses_17079) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_52_layer_call_and_return_conditional_losses_36071) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_43_layer_call_and_return_conditional_losses_21259) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_10_layer_call_and_return_conditional_losses_13999) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_50_layer_call_and_return_conditional_losses_22799) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_4_layer_call_and_return_conditional_losses_12679) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_51_layer_call_and_return_conditional_losses_35949) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_4_layer_call_and_return_conditional_losses_30215) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_32_layer_call_and_return_conditional_losses_18839) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___28147) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_16_layer_call_and_return_conditional_losses_31679) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_22_layer_call_and_return_conditional_losses_16639) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_17_layer_call_and_return_conditional_losses_15539) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_46_layer_call_and_return_conditional_losses_35339) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_35_layer_call_and_return_conditional_losses_33997) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_43_layer_call_and_return_conditional_losses_34973) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_29_layer_call_and_return_conditional_losses_33265) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_15_layer_call_and_return_conditional_losses_31557) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_19_layer_call_and_return_conditional_losses_32045) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_7_layer_call_and_return_conditional_losses_13339) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_41_layer_call_and_return_conditional_losses_34729) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_21_layer_call_and_return_conditional_losses_32289) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_16_layer_call_and_return_conditional_losses_15319) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_14_layer_call_and_return_conditional_losses_14879) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_40_layer_call_and_return_conditional_losses_34607) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_layer_call_and_return_conditional_losses_11787) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_19_layer_call_and_return_conditional_losses_15979) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_50_layer_call_and_return_conditional_losses_35827) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_26_layer_call_and_return_conditional_losses_17519) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_8_layer_call_and_return_conditional_losses_30703) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_22_layer_call_and_return_conditional_losses_32411) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_51_layer_call_and_return_conditional_losses_23019) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_31_layer_call_and_return_conditional_losses_18619) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_46_layer_call_and_return_conditional_losses_21919) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_7_layer_call_and_return_conditional_losses_30581) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_39_layer_call_and_return_conditional_losses_20379) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_17_layer_call_and_return_conditional_losses_31801) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_54_layer_call_and_return_conditional_losses_29624) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_52_layer_call_and_return_conditional_losses_23239) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_5_layer_call_and_return_conditional_losses_12899) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_37_layer_call_and_return_conditional_losses_34241) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_5_layer_call_and_return_conditional_losses_30337) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_34_layer_call_and_return_conditional_losses_33875) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_38_layer_call_and_return_conditional_losses_20159) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_9_layer_call_and_return_conditional_losses_30825) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_6_layer_call_and_return_conditional_losses_13119) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_47_layer_call_and_return_conditional_losses_35461) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_32_layer_call_and_return_conditional_losses_33631) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_36_layer_call_and_return_conditional_losses_34119) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_13_layer_call_and_return_conditional_losses_14659) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_37_layer_call_and_return_conditional_losses_19939) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_33_layer_call_and_return_conditional_losses_33753) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_8_layer_call_and_return_conditional_losses_13559) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_53_layer_call_and_return_conditional_losses_23437) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_49_layer_call_and_return_conditional_losses_22579) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_13_layer_call_and_return_conditional_losses_31313) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_18_layer_call_and_return_conditional_losses_31923) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_47_layer_call_and_return_conditional_losses_22139) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_45_layer_call_and_return_conditional_losses_35217) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_27_layer_call_and_return_conditional_losses_33021) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_14_layer_call_and_return_conditional_losses_31435) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_36_layer_call_and_return_conditional_losses_19719) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_27_layer_call_and_return_conditional_losses_17739) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_20_layer_call_and_return_conditional_losses_32167) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_1_layer_call_and_return_conditional_losses_29849) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_55_layer_call_and_return_conditional_losses_23827) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_12_layer_call_and_return_conditional_losses_14439) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_35_layer_call_and_return_conditional_losses_19499) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_34_layer_call_and_return_conditional_losses_19279) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_2_layer_call_and_return_conditional_losses_12239) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_1_layer_call_and_return_conditional_losses_12019) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_53_layer_call_and_return_conditional_losses_29513) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_12_layer_call_and_return_conditional_losses_31191) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_38_layer_call_and_return_conditional_losses_34363) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_11_layer_call_and_return_conditional_losses_31069) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_48_layer_call_and_return_conditional_losses_35583) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_55_layer_call_and_return_conditional_losses_29733) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_41_layer_call_and_return_conditional_losses_20819) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_3_layer_call_and_return_conditional_losses_12459) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_2_layer_call_and_return_conditional_losses_29971) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_9_layer_call_and_return_conditional_losses_13779) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_23_layer_call_and_return_conditional_losses_32533) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_54_layer_call_and_return_conditional_losses_23635) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_28_layer_call_and_return_conditional_losses_33143) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_39_layer_call_and_return_conditional_losses_34485) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_26_layer_call_and_return_conditional_losses_32899) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_30_layer_call_and_return_conditional_losses_33387) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_15_layer_call_and_return_conditional_losses_15099) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_45_layer_call_and_return_conditional_losses_21699) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_49_layer_call_and_return_conditional_losses_35705) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_18_layer_call_and_return_conditional_losses_15759) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_3_layer_call_and_return_conditional_losses_30093) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_44_layer_call_and_return_conditional_losses_21479) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SimCLR Output Analysis ===\n",
            "\n",
            "\n",
            "Spatial Reduction Path:\n",
            "initial_conv    shape: (1, 112, 112, 64) \n",
            "initial_max_pool shape: (1, 56, 56, 64) \n",
            "block_group1    shape: (1, 56, 56, 256) \n",
            "block_group2    shape: (1, 28, 28, 512) \n",
            "block_group3    shape: (1, 14, 14, 1024) \n",
            "block_group4    shape: (1, 7, 7, 2048) \n",
            "final_avg_pool  shape: (1, 2048) \n",
            "logits_sup      shape: (1, 1000) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loader\n",
        "\n",
        "This session defines the data loader class for the model training. One thing to note is that batch normalization is not currently supported because I haven't realized the best way to preprocess the data for batch training. Images are resized to 1/4 of resolution to fasten the training."
      ],
      "metadata": {
        "id": "CezTP6S7qrN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FirefightingDataset:\n",
        "    def __init__(self, csv_path, img_dir, image_resize_pct=1.0):\n",
        "        \"\"\"Create a metadata object for image loading. Call create_dataset() for data pipeline.\"\"\"\n",
        "        # Read CSV\n",
        "        self.df = pd.read_csv(csv_path, names=[\n",
        "            'filename', 'width', 'height', 'class',\n",
        "            'xmin', 'ymin', 'xmax', 'ymax'\n",
        "        ], skiprows=1)\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.image_resize_pct = image_resize_pct\n",
        "        self.image_ids = self.df['filename'].unique()\n",
        "\n",
        "        self.classes = sorted(self.df['class'].unique())\n",
        "        self.class_to_id = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        self.num_classes = len(self.classes)\n",
        "\n",
        "    def load_image(self, image_id):\n",
        "        \"\"\"Load and preprocess image maintaining aspect ratio\"\"\"\n",
        "        # Read image\n",
        "        img_path = os.path.join(self.img_dir, image_id)\n",
        "        image = tf.io.read_file(img_path)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "\n",
        "        # Get original size\n",
        "        orig_height = tf.cast(tf.shape(image)[0], tf.float32)\n",
        "        orig_width = tf.cast(tf.shape(image)[1], tf.float32)\n",
        "\n",
        "        # Calculate new size maintaining aspect ratio\n",
        "        scale = self.image_resize_pct\n",
        "        new_height = tf.cast(orig_height * scale, tf.int32)\n",
        "        new_width = tf.cast(orig_width * scale, tf.int32)\n",
        "\n",
        "        # Resize\n",
        "        image = tf.image.resize(image, [new_height, new_width])\n",
        "\n",
        "        # Normalize\n",
        "        image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "        return image, (orig_height, orig_width)\n",
        "\n",
        "    def get_boxes(self, image_id, orig_size):\n",
        "        \"\"\"Get normalized boxes and classes for an image\"\"\"\n",
        "        # Get annotations for this image\n",
        "        annotations = self.df[self.df['filename'] == image_id]\n",
        "\n",
        "        boxes = []\n",
        "        classes = []\n",
        "\n",
        "        orig_height, orig_width = orig_size\n",
        "\n",
        "        for _, row in annotations.iterrows():\n",
        "            # Normalize box coordinates\n",
        "            xmin = row['xmin'] / orig_width\n",
        "            ymin = row['ymin'] / orig_height\n",
        "            xmax = row['xmax'] / orig_width\n",
        "            ymax = row['ymax'] / orig_height\n",
        "\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            classes.append(self.class_to_id[row['class']])\n",
        "\n",
        "        return np.array(boxes, dtype=np.float32), np.array(classes, dtype=np.int32)\n",
        "\n",
        "    def create_dataloader(self, batch_size=1):\n",
        "        \"\"\"Create tf.data.Dataset with dynamic shapes\"\"\"\n",
        "\n",
        "        if batch_size > 1:\n",
        "            raise ValueError(\"Batch size greater than 1 needs improved data preprocessing.\")\n",
        "\n",
        "        def generator():\n",
        "            for image_id in self.image_ids:\n",
        "                # Load image\n",
        "                image, orig_size = self.load_image(image_id)\n",
        "\n",
        "                # Get boxes and classes\n",
        "                boxes, classes = self.get_boxes(image_id, orig_size)\n",
        "\n",
        "                # Create targets dict\n",
        "                targets = {\n",
        "                    'boxes': boxes,\n",
        "                    'classes': classes\n",
        "                }\n",
        "\n",
        "                yield image, targets\n",
        "\n",
        "        # Create dataset with dynamic image dimensions\n",
        "        dataset = tf.data.Dataset.from_generator(\n",
        "            generator,\n",
        "            output_signature=(\n",
        "                tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32),\n",
        "                {\n",
        "                    'boxes': tf.TensorSpec(shape=(None, 4), dtype=tf.float32),\n",
        "                    'classes': tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Batch and prefetch\n",
        "        dataset = dataset.batch(batch_size)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "# train_dataset = FirefightingDataset(\n",
        "#     csv_path='/content/Firefighting_Device_Detection/train/_annotations.csv',\n",
        "#     img_dir='/content/Firefighting_Device_Detection/train',\n",
        "# )\n",
        "# train_loader = train_dataset.create_dataloader()\n",
        "\n",
        "# valid_loader = FirefightingDataset(\n",
        "#     csv_path='/content/Firefighting_Device_Detection/valid/_annotations.csv',\n",
        "#     img_dir='/content/Firefighting_Device_Detection/valid',\n",
        "# ).create_dataloader()"
      ],
      "metadata": {
        "id": "ulaIg3WeXux4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detection BackBone\n",
        "\n",
        "We will use SimCLR feature extractor as a backbone for our detection."
      ],
      "metadata": {
        "id": "X19WKsNVrrw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DetectionBackbone(tf.keras.Model):\n",
        "    def __init__(self, simclr_model):\n",
        "        super(DetectionBackbone, self).__init__()\n",
        "        self.backbone = simclr_model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        features = self.backbone(inputs, trainable=False)\n",
        "\n",
        "        return {\n",
        "            # 'C1': features['initial_conv'],\n",
        "            # 'C2': features['initial_max_pool'],\n",
        "            'P2': features['block_group1'],\n",
        "            'P3': features['block_group2'],\n",
        "            'P4': features['block_group3'],\n",
        "            'P5': features['block_group4']\n",
        "        }\n",
        "\n",
        "# backbone = DetectionBackbone(base_model)"
      ],
      "metadata": {
        "id": "c0I7Hr4bQZ-m"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Pyramid Network\n",
        "\n",
        "A feature pyramid network based on SimCLR outputs.\n"
      ],
      "metadata": {
        "id": "3JwOO1IFsGDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FPN(tf.keras.layers.Layer):\n",
        "    def __init__(self, out_channels=256):\n",
        "        super(FPN, self).__init__()\n",
        "\n",
        "        # 1x1 convo to reduce channels\n",
        "        self.conv1_p5 = tf.keras.layers.Conv2D(out_channels, 1)\n",
        "        self.conv1_p4 = tf.keras.layers.Conv2D(out_channels, 1)\n",
        "        self.conv1_p3 = tf.keras.layers.Conv2D(out_channels, 1)\n",
        "        self.conv1_p2 = tf.keras.layers.Conv2D(out_channels, 1)\n",
        "\n",
        "        # 3x3 convo to smooth features\n",
        "        self.smooth_p5 = tf.keras.layers.Conv2D(out_channels, 3, padding='same')\n",
        "        self.smooth_p4 = tf.keras.layers.Conv2D(out_channels, 3, padding='same')\n",
        "        self.smooth_p3 = tf.keras.layers.Conv2D(out_channels, 3, padding='same')\n",
        "        self.smooth_p2 = tf.keras.layers.Conv2D(out_channels, 3, padding='same')\n",
        "\n",
        "    def call(self, features):\n",
        "        # Get features from SimCLR\n",
        "        p5 = features['block_group4']\n",
        "        p4 = features['block_group3']\n",
        "        p3 = features['block_group2']\n",
        "        p2 = features['block_group1']\n",
        "\n",
        "        # Top-down pathway\n",
        "        p5_out = self.conv1_p5(p5)\n",
        "\n",
        "        p4_out = self.conv1_p4(p4)\n",
        "        p4_out = p4_out + tf.image.resize(p5_out, tf.shape(p4_out)[1:3])\n",
        "\n",
        "        p3_out = self.conv1_p3(p3)\n",
        "        p3_out = p3_out + tf.image.resize(p4_out, tf.shape(p3_out)[1:3])\n",
        "\n",
        "        p2_out = self.conv1_p2(p2)\n",
        "        p2_out = p2_out + tf.image.resize(p3_out, tf.shape(p2_out)[1:3])\n",
        "\n",
        "        # Final smooth\n",
        "        return {\n",
        "            'P5': self.smooth_p5(p5_out),\n",
        "            'P4': self.smooth_p4(p4_out),\n",
        "            'P3': self.smooth_p3(p3_out),\n",
        "            'P2': self.smooth_p2(p2_out)\n",
        "        }"
      ],
      "metadata": {
        "id": "0h2BO1QNXVIZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detection Head\n",
        "\n",
        "Detection Head implementation that will be used with each layer of FPN; composed of a classification branch and a box regression branch."
      ],
      "metadata": {
        "id": "tyGeZHxdsk_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DetectionHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DetectionHead, self).__init__()\n",
        "\n",
        "        self.cls_conv = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
        "            tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
        "            tf.keras.layers.Conv2D(num_classes, 3, padding='same')\n",
        "        ])\n",
        "\n",
        "        self.box_conv = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
        "            tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
        "            tf.keras.layers.Conv2D(4, 3, padding='same')  # (x, y, w, h)\n",
        "        ])\n",
        "\n",
        "    def call(self, features):\n",
        "        return {\n",
        "            'cls_logits': self.cls_conv(features),\n",
        "            'box_pred': self.box_conv(features)\n",
        "        }"
      ],
      "metadata": {
        "id": "W1ldNRcPZPkR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detection Model\n",
        "\n",
        "SimCLR, FPN and detection heads pipelines are constructed as part of the model."
      ],
      "metadata": {
        "id": "PSrfWj8cuGMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DetectionModel(tf.keras.Model):\n",
        "    def __init__(self, simclr_model, num_classes):\n",
        "        super(DetectionModel, self).__init__()\n",
        "\n",
        "        # SimCLR backbone\n",
        "        self.backbone = simclr_model\n",
        "\n",
        "        #FPN\n",
        "        self.fpn = FPN()\n",
        "\n",
        "        #Detection heads\n",
        "        self.detection_heads = {\n",
        "            'P2': DetectionHead(num_classes),\n",
        "            'P3': DetectionHead(num_classes),\n",
        "            'P4': DetectionHead(num_classes),\n",
        "            'P5': DetectionHead(num_classes)\n",
        "        }\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Backbone -> fpn -> detection heads\n",
        "        features = self.backbone(inputs, trainable=False)\n",
        "        fpn_features = self.fpn(features)\n",
        "        predictions = {}\n",
        "        for level in ['P2', 'P3', 'P4', 'P5']:\n",
        "            predictions[level] = self.detection_heads[level](fpn_features[level])\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "pvFGpkI5XWL1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function\n",
        "\n"
      ],
      "metadata": {
        "id": "4mDUivlyutG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DetectionLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cls_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        self.box_loss_fn = tf.keras.losses.Huber()\n",
        "\n",
        "        # Class frequencies from dataset analysis (train split)\n",
        "        self.class_frequencies = {\n",
        "            '24V-power-cord': 0.361,\n",
        "            'fire-fan-manual-control-line': 0.131,\n",
        "            'i-o-module': 0.058,\n",
        "            'bus-isolation-module': 0.052,\n",
        "            'coded-smoke-detector': 0.051,\n",
        "            'fire-hydrant-button': 0.040,\n",
        "            'acousto-optic-alarm': 0.036,\n",
        "            'manual-alarm-button-with-fire-telephone-jack': 0.033,\n",
        "            'manual-automatic-switching-device': 0.023,\n",
        "            'coded-temperature-detector': 0.022,\n",
        "            'input-module': 0.015,\n",
        "            'normally-open-smoke-exhaust-valve-with-280-operation': 0.013,\n",
        "            'fire-broadcasting-line': 0.013,\n",
        "            'secondary-fire-shutter-door-control-box': 0.013,\n",
        "            'dedicated-metal-module-box-for-fire-smoke-exhaust-fan': 0.012,\n",
        "            'fire-water-pump-manual-control-line': 0.012,\n",
        "            'light-display': 0.012,\n",
        "            'security-video-intercom-door-machine': 0.010,\n",
        "            'normally-open-smoke-exhaust-valve-with-70-operation': 0.010,\n",
        "            'dedicated-metal-module-box-for-fire-supplementary-fan': 0.009,\n",
        "            'water-flow-indicator': 0.007,\n",
        "            'safety-signal-valve': 0.007,\n",
        "            'speaker': 0.006,\n",
        "            'area-display': 0.006,\n",
        "            'fire-equipment-power-monitoring-line': 0.006,\n",
        "            'voltage-signal-sensor': 0.005,\n",
        "            'fire-telephone-extension': 0.005,\n",
        "            'video-intercom-card-reader': 0.005,\n",
        "            'gun-type-infrared-camera-in-the-basement': 0.005,\n",
        "            'metal-modular-box': 0.003,\n",
        "            'pressure-switch-gas-extinguisher': 0.003,\n",
        "            'the-electromagnetic-valve': 0.003,\n",
        "            'deflation-indicator-light': 0.003,\n",
        "            'gas-spray-audible-and-visual-alarm': 0.003,\n",
        "            'pressure-switch-flow-switch-start-pump-line': 0.003,\n",
        "            'smoke-vent': 0.003,\n",
        "            'electrical-fire-monitoring-line': 0.002,\n",
        "            'emergency-manual-start-stop-button': 0.002,\n",
        "            'normally-open-smoke-exhaust-valve-with-70-operation-closed-in-case-of-fire': 0.002,\n",
        "            'dedicated-metal-module-box-for-fire-pump': 0.001\n",
        "        }\n",
        "\n",
        "        # Calculate inverse frequency weights\n",
        "        max_freq = max(self.class_frequencies.values())\n",
        "        self.class_weights = {\n",
        "            idx: max_freq / freq\n",
        "            for idx, (_, freq) in enumerate(self.class_frequencies.items())\n",
        "        }\n",
        "\n",
        "        # Normalize weights to have mean of 1\n",
        "        weight_mean = sum(self.class_weights.values()) / len(self.class_weights)\n",
        "        self.class_weights = {\n",
        "            k: v / weight_mean for k, v in self.class_weights.items()\n",
        "        }\n",
        "\n",
        "        print(\"Class weights:\")\n",
        "        for idx, (class_name, weight) in enumerate(zip(self.class_frequencies.keys(), self.class_weights.values())):\n",
        "            print(f\"{idx}: {class_name}: {weight:.2f}\")\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        gt_boxes = y_true['boxes']\n",
        "        gt_classes = y_true['classes']\n",
        "        #print(gt_classes)\n",
        "        gt_classes = tf.cast(gt_classes, tf.int32)\n",
        "\n",
        "        total_loss = 0.0\n",
        "        num_levels = len(['P2', 'P3', 'P4', 'P5'])\n",
        "\n",
        "        for level in ['P2', 'P3', 'P4', 'P5']:\n",
        "            level_preds = y_pred[level]\n",
        "\n",
        "            pred_cls = level_preds['cls_logits']\n",
        "            pred_box = level_preds['box_pred']\n",
        "\n",
        "            # Reshape predictions\n",
        "            pred_cls = tf.reshape(pred_cls, [1, -1, len(self.class_frequencies)])\n",
        "            pred_box = tf.reshape(pred_box, [1, -1, 4])\n",
        "\n",
        "            # IoU calculation\n",
        "            iou = self._compute_iou(\n",
        "                tf.squeeze(pred_box, axis=0),\n",
        "                tf.squeeze(gt_boxes, axis=0)\n",
        "            )\n",
        "\n",
        "            # Get best predictions for each ground truth\n",
        "            best_idx = tf.argmax(iou, axis=0)\n",
        "\n",
        "            # Gather best predictions\n",
        "            batch_idx = tf.zeros_like(best_idx)\n",
        "            gather_idx = tf.stack([batch_idx, best_idx], axis=1)\n",
        "\n",
        "            matched_cls = tf.gather_nd(pred_cls, gather_idx)\n",
        "            matched_box = tf.gather_nd(pred_box, gather_idx)\n",
        "\n",
        "            # Get class weights for ground truth classes\n",
        "            class_weights = tf.gather(\n",
        "                list(self.class_weights.values()),\n",
        "                tf.squeeze(gt_classes, axis=0)\n",
        "            )\n",
        "\n",
        "            # Calculate weighted classification loss\n",
        "            cls_loss = self.cls_loss_fn(\n",
        "                tf.squeeze(gt_classes, axis=0),\n",
        "                matched_cls\n",
        "            )\n",
        "            weighted_cls_loss = cls_loss * class_weights\n",
        "\n",
        "            # Calculate box loss (not weighted)\n",
        "            box_loss = self.box_loss_fn(\n",
        "                tf.squeeze(gt_boxes, axis=0),\n",
        "                matched_box\n",
        "            )\n",
        "\n",
        "            # Combine losses\n",
        "            level_loss = tf.reduce_mean(weighted_cls_loss) + box_loss\n",
        "            total_loss += level_loss / num_levels\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def _compute_iou(self, boxes1, boxes2):\n",
        "        \"\"\"Compute IoU between two sets of boxes\"\"\"\n",
        "        # intersection coordinates\n",
        "        x1 = tf.maximum(boxes1[:, None, 0], boxes2[None, :, 0])\n",
        "        y1 = tf.maximum(boxes1[:, None, 1], boxes2[None, :, 1])\n",
        "        x2 = tf.minimum(boxes1[:, None, 2], boxes2[None, :, 2])\n",
        "        y2 = tf.minimum(boxes1[:, None, 3], boxes2[None, :, 3])\n",
        "\n",
        "        # intersection\n",
        "        intersection = tf.maximum(0.0, x2 - x1) * tf.maximum(0.0, y2 - y1)\n",
        "\n",
        "        # box areas\n",
        "        area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
        "        area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
        "\n",
        "        # union\n",
        "        union = area1[:, None] + area2[None, :] - intersection\n",
        "\n",
        "        return intersection / (union + 1e-7)"
      ],
      "metadata": {
        "id": "RPkEecD_XiTW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eN4lm6FfuFL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, learning_rate=1e-4):\n",
        "        self.model = model\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "        self.loss_fn = DetectionLoss()\n",
        "\n",
        "        # metrics for train and validation\n",
        "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "        self.val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "\n",
        "        # Early stopping\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.patience = 5\n",
        "        self.patience_counter = 0\n",
        "\n",
        "        # Training history\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'val_loss': []\n",
        "        }\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def train_step(self, images, targets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.model(images, training=True)\n",
        "            loss = self.loss_fn(targets, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "        self.train_loss.update_state(loss)\n",
        "        return loss\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def val_step(self, images, targets):\n",
        "        predictions = self.model(images, training=False)\n",
        "        loss = self.loss_fn(targets, predictions)\n",
        "        self.val_loss.update_state(loss)\n",
        "        return loss\n",
        "\n",
        "    def train(self, train_dataloader, val_dataloader, epochs=10):\n",
        "\n",
        "        # Create checkpoint directory if it doesn't exist\n",
        "        checkpoint_dir = 'checkpoints'\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "            self.train_loss.reset_state()\n",
        "            self.val_loss.reset_state()\n",
        "\n",
        "            # Training loop\n",
        "            progress_bar = tf.keras.utils.Progbar(\n",
        "                len(list(train_dataloader)),\n",
        "                stateful_metrics=['loss']\n",
        "            )\n",
        "\n",
        "            for step, (images, targets) in enumerate(train_dataloader):\n",
        "                loss = self.train_step(images, targets)\n",
        "                progress_bar.update(\n",
        "                    step + 1,\n",
        "                    values=[('loss', self.train_loss.result())]\n",
        "                )\n",
        "\n",
        "            # validate with the valid data\n",
        "            for val_images, val_targets in val_dataloader:\n",
        "                self.val_step(val_images, val_targets)\n",
        "\n",
        "            # update history\n",
        "            train_loss = self.train_loss.result()\n",
        "            val_loss = self.val_loss.result()\n",
        "            self.history['train_loss'].append(train_loss.numpy())\n",
        "            self.history['val_loss'].append(val_loss.numpy())\n",
        "\n",
        "            # epoch results\n",
        "            print(f\"\\nEpoch {epoch + 1}\")\n",
        "            print(f\"Training Loss: {train_loss:.4f}\")\n",
        "            print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.patience_counter = 0\n",
        "                print(\"Saving best model...\")\n",
        "                save_path = os.path.join(checkpoint_dir, f'best_model_epoch_{epoch+1}.weights.h5')\n",
        "                self.model.save_weights(save_path)\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "                if self.patience_counter >= self.patience:\n",
        "                    print(\"\\nEarly stopping triggered!\")\n",
        "                    break"
      ],
      "metadata": {
        "id": "5VS1OcAiXjbt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = FirefightingDataset(\n",
        "    csv_path='/content/Firefighting_Device_Detection/train/_annotations.csv',\n",
        "    img_dir='/content/Firefighting_Device_Detection/train',)\n",
        "\n",
        "train_dataloader = train_dataset.create_dataloader()\n",
        "# train_loader = train_dataset.create_dataloader()\n",
        "\n",
        "val_dataset = FirefightingDataset(\n",
        "    csv_path='/content/Firefighting_Device_Detection/valid/_annotations.csv',\n",
        "    img_dir='/content/Firefighting_Device_Detection/valid',\n",
        ")\n",
        "\n",
        "val_dataloader = val_dataset.create_dataloader()\n",
        "\n",
        "def train_detector():\n",
        "\n",
        "    base_model = hub.load(\"gs://simclr-checkpoints-tf2/simclrv2/pretrained/r50_1x_sk0/saved_model/\")\n",
        "    detector = DetectionModel(\n",
        "        simclr_model=base_model,\n",
        "        num_classes=train_dataset.num_classes\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(detector)\n",
        "\n",
        "    history = trainer.train(\n",
        "        train_dataloader=train_dataloader,\n",
        "        val_dataloader=val_dataloader,\n",
        "        epochs=10\n",
        "    )\n",
        "\n",
        "    #trainer.train(train_loader, epochs=10)\n",
        "\n",
        "    return detector\n",
        "\n",
        "# start training\n",
        "model = train_detector()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iKuZ_3xY2O1",
        "outputId": "2b20fb29-859b-4029-e3c0-eba36c36ab9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_42_layer_call_and_return_conditional_losses_34851) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_48_layer_call_and_return_conditional_losses_22359) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_25_layer_call_and_return_conditional_losses_32777) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_20_layer_call_and_return_conditional_losses_16199) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_30_layer_call_and_return_conditional_losses_18399) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_layer_call_and_return_conditional_losses_29402) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_33_layer_call_and_return_conditional_losses_19059) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_21_layer_call_and_return_conditional_losses_16419) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_25_layer_call_and_return_conditional_losses_17299) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_10_layer_call_and_return_conditional_losses_30947) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_31_layer_call_and_return_conditional_losses_33509) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_42_layer_call_and_return_conditional_losses_21039) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_40_layer_call_and_return_conditional_losses_20599) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_6_layer_call_and_return_conditional_losses_30459) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_24_layer_call_and_return_conditional_losses_32655) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_28_layer_call_and_return_conditional_losses_17959) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_11_layer_call_and_return_conditional_losses_14219) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_29_layer_call_and_return_conditional_losses_18179) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_44_layer_call_and_return_conditional_losses_35095) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_23_layer_call_and_return_conditional_losses_16859) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_24_layer_call_and_return_conditional_losses_17079) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_52_layer_call_and_return_conditional_losses_36071) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_43_layer_call_and_return_conditional_losses_21259) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_10_layer_call_and_return_conditional_losses_13999) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_50_layer_call_and_return_conditional_losses_22799) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_4_layer_call_and_return_conditional_losses_12679) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_51_layer_call_and_return_conditional_losses_35949) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_4_layer_call_and_return_conditional_losses_30215) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_32_layer_call_and_return_conditional_losses_18839) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___28147) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_16_layer_call_and_return_conditional_losses_31679) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_22_layer_call_and_return_conditional_losses_16639) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_17_layer_call_and_return_conditional_losses_15539) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_46_layer_call_and_return_conditional_losses_35339) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_35_layer_call_and_return_conditional_losses_33997) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_43_layer_call_and_return_conditional_losses_34973) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_29_layer_call_and_return_conditional_losses_33265) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_15_layer_call_and_return_conditional_losses_31557) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_19_layer_call_and_return_conditional_losses_32045) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_7_layer_call_and_return_conditional_losses_13339) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_41_layer_call_and_return_conditional_losses_34729) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_21_layer_call_and_return_conditional_losses_32289) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_16_layer_call_and_return_conditional_losses_15319) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_14_layer_call_and_return_conditional_losses_14879) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_40_layer_call_and_return_conditional_losses_34607) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_layer_call_and_return_conditional_losses_11787) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_19_layer_call_and_return_conditional_losses_15979) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_50_layer_call_and_return_conditional_losses_35827) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_26_layer_call_and_return_conditional_losses_17519) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_8_layer_call_and_return_conditional_losses_30703) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_22_layer_call_and_return_conditional_losses_32411) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_51_layer_call_and_return_conditional_losses_23019) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_31_layer_call_and_return_conditional_losses_18619) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_46_layer_call_and_return_conditional_losses_21919) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_7_layer_call_and_return_conditional_losses_30581) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_39_layer_call_and_return_conditional_losses_20379) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_17_layer_call_and_return_conditional_losses_31801) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_54_layer_call_and_return_conditional_losses_29624) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_52_layer_call_and_return_conditional_losses_23239) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_5_layer_call_and_return_conditional_losses_12899) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_37_layer_call_and_return_conditional_losses_34241) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_5_layer_call_and_return_conditional_losses_30337) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_34_layer_call_and_return_conditional_losses_33875) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_38_layer_call_and_return_conditional_losses_20159) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_9_layer_call_and_return_conditional_losses_30825) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_6_layer_call_and_return_conditional_losses_13119) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_47_layer_call_and_return_conditional_losses_35461) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_32_layer_call_and_return_conditional_losses_33631) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_36_layer_call_and_return_conditional_losses_34119) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_13_layer_call_and_return_conditional_losses_14659) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_37_layer_call_and_return_conditional_losses_19939) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_33_layer_call_and_return_conditional_losses_33753) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_8_layer_call_and_return_conditional_losses_13559) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_53_layer_call_and_return_conditional_losses_23437) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_49_layer_call_and_return_conditional_losses_22579) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_13_layer_call_and_return_conditional_losses_31313) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_18_layer_call_and_return_conditional_losses_31923) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_47_layer_call_and_return_conditional_losses_22139) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_45_layer_call_and_return_conditional_losses_35217) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_27_layer_call_and_return_conditional_losses_33021) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_14_layer_call_and_return_conditional_losses_31435) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_36_layer_call_and_return_conditional_losses_19719) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_27_layer_call_and_return_conditional_losses_17739) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_20_layer_call_and_return_conditional_losses_32167) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_1_layer_call_and_return_conditional_losses_29849) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_55_layer_call_and_return_conditional_losses_23827) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_12_layer_call_and_return_conditional_losses_14439) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_35_layer_call_and_return_conditional_losses_19499) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_34_layer_call_and_return_conditional_losses_19279) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_2_layer_call_and_return_conditional_losses_12239) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_1_layer_call_and_return_conditional_losses_12019) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_53_layer_call_and_return_conditional_losses_29513) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_12_layer_call_and_return_conditional_losses_31191) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_38_layer_call_and_return_conditional_losses_34363) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_11_layer_call_and_return_conditional_losses_31069) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_48_layer_call_and_return_conditional_losses_35583) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_55_layer_call_and_return_conditional_losses_29733) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_41_layer_call_and_return_conditional_losses_20819) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_3_layer_call_and_return_conditional_losses_12459) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_2_layer_call_and_return_conditional_losses_29971) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_9_layer_call_and_return_conditional_losses_13779) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_23_layer_call_and_return_conditional_losses_32533) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_54_layer_call_and_return_conditional_losses_23635) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_28_layer_call_and_return_conditional_losses_33143) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_39_layer_call_and_return_conditional_losses_34485) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_26_layer_call_and_return_conditional_losses_32899) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_30_layer_call_and_return_conditional_losses_33387) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_15_layer_call_and_return_conditional_losses_15099) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_45_layer_call_and_return_conditional_losses_21699) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_49_layer_call_and_return_conditional_losses_35705) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_18_layer_call_and_return_conditional_losses_15759) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_3_layer_call_and_return_conditional_losses_30093) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sync_batch_normalization_44_layer_call_and_return_conditional_losses_21479) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights:\n",
            "0: 24V-power-cord: 0.01\n",
            "1: fire-fan-manual-control-line: 0.04\n",
            "2: i-o-module: 0.09\n",
            "3: bus-isolation-module: 0.10\n",
            "4: coded-smoke-detector: 0.10\n",
            "5: fire-hydrant-button: 0.13\n",
            "6: acousto-optic-alarm: 0.15\n",
            "7: manual-alarm-button-with-fire-telephone-jack: 0.16\n",
            "8: manual-automatic-switching-device: 0.23\n",
            "9: coded-temperature-detector: 0.24\n",
            "10: input-module: 0.35\n",
            "11: normally-open-smoke-exhaust-valve-with-280-operation: 0.41\n",
            "12: fire-broadcasting-line: 0.41\n",
            "13: secondary-fire-shutter-door-control-box: 0.41\n",
            "14: dedicated-metal-module-box-for-fire-smoke-exhaust-fan: 0.44\n",
            "15: fire-water-pump-manual-control-line: 0.44\n",
            "16: light-display: 0.44\n",
            "17: security-video-intercom-door-machine: 0.53\n",
            "18: normally-open-smoke-exhaust-valve-with-70-operation: 0.53\n",
            "19: dedicated-metal-module-box-for-fire-supplementary-fan: 0.59\n",
            "20: water-flow-indicator: 0.76\n",
            "21: safety-signal-valve: 0.76\n",
            "22: speaker: 0.89\n",
            "23: area-display: 0.89\n",
            "24: fire-equipment-power-monitoring-line: 0.89\n",
            "25: voltage-signal-sensor: 1.06\n",
            "26: fire-telephone-extension: 1.06\n",
            "27: video-intercom-card-reader: 1.06\n",
            "28: gun-type-infrared-camera-in-the-basement: 1.06\n",
            "29: metal-modular-box: 1.77\n",
            "30: pressure-switch-gas-extinguisher: 1.77\n",
            "31: the-electromagnetic-valve: 1.77\n",
            "32: deflation-indicator-light: 1.77\n",
            "33: gas-spray-audible-and-visual-alarm: 1.77\n",
            "34: pressure-switch-flow-switch-start-pump-line: 1.77\n",
            "35: smoke-vent: 1.77\n",
            "36: electrical-fire-monitoring-line: 2.66\n",
            "37: emergency-manual-start-stop-button: 2.66\n",
            "38: normally-open-smoke-exhaust-valve-with-70-operation-closed-in-case-of-fire: 2.66\n",
            "39: dedicated-metal-module-box-for-fire-pump: 5.32\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m 38/102\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:48\u001b[0m 3s/step - loss: 1.7458"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JwiGD5Eq3jyP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TF2 finetuning.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}